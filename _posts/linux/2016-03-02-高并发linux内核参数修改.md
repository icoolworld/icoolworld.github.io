---
layout: post
title: 高并发linux内核参数修改
categories: linux
---

# 高并发linux内核参数调优
```
#【net】
######################## cat /proc/sys/net/ipv4/tcp_syncookies
# 默认值：1
# 作用：是否打开SYN Cookie功能，该功能可以防止部分SYN攻击
net.ipv4.tcp_syncookies = 1
 
######################## cat /proc/sys/net/ipv4/ip_local_port_range
# 默认值：32768   61000
# 作用：可用端口的范围
net.ipv4.ip_local_port_range = 1024  65535
 
######################## cat /proc/sys/net/ipv4/tcp_fin_timeout 
# 默认值：60
# 作用：TCP时间戳
net.ipv4.tcp_fin_timeout = 30
 
######################## cat /proc/sys/net/ipv4/tcp_timestamps 
# 默认值：1
# 作用：TCP时间戳
net.ipv4.tcp_timestamps = 1
 
######################## cat /proc/sys/net/ipv4/tcp_tw_recycle
# 默认值：0
# 作用：针对TIME-WAIT，不要开启。不少文章提到同时开启tcp_tw_recycle和tcp_tw_reuse，会带来C/S在NAT方面的异常
# 个人接受的做法是，开启tcp_tw_reuse，增加ip_local_port_range的范围，减小tcp_max_tw_buckets和tcp_fin_timeout的值
# 参考：http://ju.outofmemory.cn/entry/91121, http://www.cnblogs.com/lulu/p/4149312.html
net.ipv4.tcp_tw_recycle = 0
 
######################## cat /proc/sys/net/ipv4/tcp_tw_reuse
# 默认值：0
# 作用：针对TIME-WAIT，做为客户端可以启用（例如，作为nginx-proxy前端代理，要访问后端的服务）
net.ipv4.tcp_tw_reuse = 1
 
######################## cat /proc/sys/net/ipv4/tcp_max_tw_buckets 
# 默认值：262144
# 作用：针对TIME-WAIT，配置其上限。如果降低这个值，可以显著的发现time-wait的数量减少，但系统日志中可能出现如下记录：
# kernel: TCP: time wait bucket table overflow
# 对应的，如果升高这个值，可以显著的发现time-wait的数量增加。
# 综合考虑，保持默认值。
net.ipv4.tcp_max_tw_buckets = 262144
 
######################## cat /proc/sys/net/ipv4/tcp_max_orphans 
# 默认值：16384
# 作用：orphans的最大值
net.ipv4.tcp_max_orphans = 3276800
 
######################## cat /proc/sys/net/ipv4/tcp_max_syn_backlog
# 默认值：128
# 作用：增大SYN队列的长度，容纳更多连接
net.ipv4.tcp_max_syn_backlog = 819200
 
######################## cat /proc/sys/net/ipv4/tcp_keepalive_intvl 
# 默认值：75
# 作用：探测失败后，间隔几秒后重新探测
net.ipv4.tcp_keepalive_intvl = 30
 
######################## cat /proc/sys/net/ipv4/tcp_keepalive_probes 
# 默认值：9
# 作用：探测失败后，最多尝试探测几次
net.ipv4.tcp_keepalive_probes = 3
 
######################## cat /proc/sys/net/ipv4/tcp_keepalive_time 
# 默认值：7200
# 作用：间隔多久发送1次keepalive探测包
net.ipv4.tcp_keepalive_time = 1200
 
######################## cat /proc/sys/net/netfilter/nf_conntrack_tcp_timeout_established
# 默认值：432000
# 作用：设置 conntrack tcp 状态的超时时间，如果系统出现下述异常时要考虑调整：
# ping: sendmsg: Operation not permitted
# kernel: nf_conntrack: table full, dropping packet.
# 参考：http://www.linuxidc.com/Linux/2012-11/75151.htm, http://blog.csdn.net/dog250/article/details/9318843
net.netfilter.nf_conntrack_tcp_timeout_established = 600
 
######################## cat /proc/sys/net/netfilter/nf_conntrack_max
# 默认值：65535
# 作用：设置 conntrack 的上限，如果系统出现下述异常时要考虑调整：
# ping: sendmsg: Operation not permitted
# kernel: nf_conntrack: table full, dropping packet.
# 参考：https://blog.yorkgu.me/2012/02/09/kernel-nf_conntrack-table-full-dropping-packet/, http://www.cnblogs.com/mydomain/archive/2013/05/19/3087153.html
net.netfilter.nf_conntrack_max = 655350
 
 
##########################################################################################
######################## cat /proc/sys/net/core/netdev_max_backlog
# 默认值：1000
# 作用：网卡设备将请求放入队列的长度
net.core.netdev_max_backlog = 500000
 
######################## cat /proc/sys/net/core/somaxconn
# 默认值：128
# 作用：已经成功建立连接的套接字将要进入队列的长度
net.core.somaxconn = 65536
 
######################## cat /proc/sys/net/core/rmem_default
# 默认值：212992
# 作用：默认的TCP数据接收窗口大小（字节）
net.core.rmem_default = 8388608
 
######################## cat /proc/sys/net/core/wmem_default
# 默认值：212992
# 作用：默认的TCP数据发送窗口大小（字节）
net.core.wmem_default = 8388608
 
######################## cat /proc/sys/net/core/rmem_max
# 默认值：212992
# 作用：最大的TCP数据接收窗口大小（字节）
net.core.rmem_max = 16777216
 
######################## cat /proc/sys/net/core/wmem_max
# 默认值：212992
# 作用：最大的TCP数据发送窗口大小（字节）
net.core.wmem_max = 16777216
 
######################## cat /proc/sys/net/ipv4/tcp_mem
# 默认值：94389   125854  188778
# 作用：内存使用的下限  警戒值  上限
net.ipv4.tcp_mem = 94500000   915000000   927000000
 
######################## cat /proc/sys/net/ipv4/tcp_rmem
# 默认值：4096    87380   6291456
# 作用：socket接收缓冲区内存使用的下限  警戒值  上限
net.ipv4.tcp_rmem = 4096   87380   16777216
 
######################## cat /proc/sys/net/ipv4/tcp_wmem
# 默认值：4096    16384   4194304
# 作用：socket发送缓冲区内存使用的下限  警戒值  上限
net.ipv4.tcp_wmem = 4096   16384   16777216
 
 
 
##########################################################################################
######################## cat /proc/sys/net/ipv4/tcp_thin_dupack
# 默认值：0
# 作用：收到dupACK时要去检查tcp stream是不是 thin ( less than 4 packets in flight) 
net.ipv4.tcp_thin_dupack = 1
 
######################## cat /proc/sys/net/ipv4/tcp_thin_linear_timeouts
# 默认值：0
# 作用：重传超时后要去检查tcp stream是不是 thin ( less than 4 packets in flight) 
net.ipv4.tcp_thin_linear_timeouts = 1
 
######################## cat /proc/sys/net/unix/max_dgram_qlen
# 默认值：10
# 作用：UDP队列里数据报的最大个数
net.unix.max_dgram_qlen = 30000
 
##########################################################################################
######################## 针对lvs，关闭网卡LRO/GRO功能
# 现在大多数网卡都具有LRO/GRO功能，即网卡收包时将同一流的小包合并成大包 （tcpdump抓包可以看到>MTU 1500bytes的数据包）交给 内核协议栈；LVS内核模块在处理>MTU的数据包时，会丢弃；
# 因此，如果我们用LVS来传输大文件，很容易出现丢包，传输速度慢；
# 解决方法，关闭LRO/GRO功能，命令：
# ethtool -k eth0 查看LRO/GRO当前是否打开
# ethtool -K eth0 lro off 关闭GRO
# ethtool -K eth0 gro off 关闭GRO
 
 
 
#【kernel】
######################## cat /proc/sys/kernel/randomize_va_space
# 默认值：2
# 作用：内核的随机地址保护模式
kernel.randomize_va_space = 1
 
######################## cat /proc/sys/kernel/panic
# 默认值：0
# 作用：内核panic时，1秒后自动重启
kernel.panic = 1
 
######################## cat /proc/sys/kernel/core_pattern
# 默认值：|/usr/libexec/abrt-hook-ccpp %s %c %p %u %g %t e
# 作用：程序生成core时的文件名格式
kernel.core_pattern = core_%e
 
######################## cat /proc/sys/kernel/sysrq
# 默认值：0
# 作用：是否启用sysrq功能
kernel.sysrq = 0
 
 
 
#【vm】
######################## cat /proc/sys/vm/min_free_kbytes 
# 默认值：8039
# 作用：保留内存的最低值
vm.min_free_kbytes=901120
 
######################## cat /proc/sys/vm/panic_on_oom 
# 默认值：0
# 作用：发生oom时，自动转换为panic
vm.panic_on_oom=1
 
######################## cat /proc/sys/vm/min_free_kbytes 
# 默认值：45056
# 作用：保留最低可用内存
vm.min_free_kbytes=1048576
 
######################## cat /proc/sys/vm/swappiness 
# 默认值：60
# 作用：数值（0-100）越高，越可能发生swap交换
vm.swappiness=20
 
 
#【fs】
######################## cat /proc/sys/fs/inotify/max_user_watches
# 默认值：8192
# 作用：inotify的watch数量
fs.inotify.max_user_watches=8192000
 
######################## cat /proc/sys/fs/aio-max-nr
# 默认值：65536
# 作用：aio最大值
fs.aio-max-nr=1048576
 
######################## cat /proc/sys/fs/file-max
# 默认值：98529
# 作用：文件描述符的最大值
fs.file-max = 1048575
```
 
ZYXW、参考
1、Linux内核高性能优化【生产环境实例】
http://yangrong.blog.51cto.com/6945369/1567427
2、linux内核参数解释说明
http://yangrong.blog.51cto.com/6945369/1321594
3、tcp_tw_reuse、tcp_tw_recycle 使用场景及注意事项
http://www.cnblogs.com/lulu/p/4149312.html


## The Backlog Queue

### net.core.somaxconn

> tcp连接内核参数调优net.core.somaxconn,超过这个数量就会导致链接超时或者触发重传机制

对于一个TCP连接，Server与Client需要通过三次握手来建立网络连接.当三次握手成功后,
我们可以看到端口的状态由LISTEN转变为ESTABLISHED,接着这条链路上就可以开始传送数据了.每一个处于监听(Listen)状态的端口,都有自己的监听队列.监听队列的长度,与如下两方面有关:
　　- somaxconn参数.
　　- 使用该端口的程序中listen()函数.

关于net.core.somaxconn参数:

定义了系统中每一个端口最大的监听队列的长度,这是个全局的参数,默认值为128.限制了每个端口接收新tcp连接侦听队列的大小。对于一个经常处理新连接的高负载 web服务环境来说，默认的 128 太小了。大多数环境这个值建议增加到 1024 或者更多。 服务进程会自己限制侦听队列的大小(例如 sendmail(8) 或者 Apache)，常常在它们的配置文件中有设置队列大小的选项。大的侦听队列对防止拒绝服务 DoS 攻击也会有所帮助。

查看参数值net.core.somaxconn,默认128
cat /proc/sys/net/core/somaxconn

立即生效
```
echo 65535 >   /proc/sys/net/core/somaxconn 
sysctl -w net.core.somaxconn=65535
```

永久生效：
```
vim /etc/sysctl.conf
net.core.somaxconn = 65535
sysctl -p
```

> 注意

如果net.core.somaxconn的值大于512，需要显示指定nginx的backlog  

```
listen 80 default backlog=65535;
```


### net.core.netdev_max_backlog = 500000
```
# 默认值：1000
# 作用：网卡设备将请求放入队列的长度
```
net.core.netdev_max_backlog - 在切换到CPU之前，网卡缓冲数据包的速率。 增加该值可以提高具有大量带宽的机器的性能。 检查内核日志中与此设置相关的错误，并查阅网卡文档以获取有关更改的建议。

 每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目
 ```
 vi /etc/sysctl.conf 
 net.core.netdev_max_backlog = 500000
 ```

## IPV4
cat /proc/sys/net/ipv4/tcp_max_orphans
net.ipv4.tcp_max_orphans = 262144

## 文件描述符 File Descriptors

文件描述符是用于表示连接和打开文件的操作系统资源等。 NGINX每个连接最多可以使用两个文件描述符。 例如，如果NGINX是代理服务器，则通常使用一个文件描述符作为客户端连接，另一个用于与代理服务器的连接，尽管如果使用HTTP keepalives，此比率要低得多。 对于服务大量连接的系统，可能需要调整以下设置：

sys.fs.file_max - 文件描述符的系统范围限制,针对整个系统
nofile - 用户文件描述符限制，在/etc/security/limits.conf文件中设置
```
cat /proc/sys/fs/file-max
vim /etc/sysctl.conf
fs.file-max = 9777067
立即生效
sysctl -p
```

ulimit -n 制进程级别能够打开的文件句柄的数量。提供对shell及其启动的进程的可用文件句柄的控制。这是进程级别的。
```
vim /etc/security/limits.conf
* soft nofile 65535
* hard nofile 65535
```
当前用户session立即生效，还需要执行：ulimit -n 65535 

## 端口相关 Ephemeral Ports
当NGINX充当代理时，与上游服务器的每个连接都使用临时或临时端口。 您可能想要更改此设置：

net.ipv4.ip_local_port_range - 端口值范围的开始和结束。 如果看到您的端口用完，请增加范围。 一个常见的设置是端口1024到65000。
```
vi /etc/sysctl.conf 
net.ipv4.ip_local_port_range = 10000   61000
```

